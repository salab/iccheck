# Evaluation using OSS Repositories

Executing files in filename order should replicate the results in the paper.

## Preparing repositories

Run `./1_clone.sh` to clone repositories.

Note that we have `./1_head.txt` data file which indicates HEAD commits of each repository
at the time of the original experiment.
In the `1_clone.sh` script, we first clone the entire repository, and then use this data file
to reset the HEAD to the original HEAD commit.

## Running ICCheck

`./2_run.sh` will run the actual experiment.
Note that this will take a long time, from a few hours to a whole day using a 24 core machine.
The script will make a directory called `./logs` and store execution results for each repository there.

## Analyzing the results for statistics

`./3_*.sh` files will analyze the results.

`./3_1_calc-repo-run-time.sh` analyzes logs in the `./logs` and output basic statistics.
Note that this script excludes commits with more than 25 changed files.
The script also outputs `./3_2_filtered.log` for usage with later analysis scripts.

`3_3_plot_repo_run_times.py` will render the "Execution time of ICCheck" figure in the paper.

`4_1_plot_detected_missing_changes.py` will render the "Number of detected missing changes" figure in the paper.

`4_3_plot_lang_info.py` will render the "Number of suggestions per programming language" figure in the paper.

## Manually reviewing the results for accuracy

We have randomly picked 100 clone sets from the results, and have manually reviewed them.

(You do not have to run here to replicate the results)
`5_1_list_clones.py` will pre-format the logs and output `5_2_clone_sets.ndjson`.
(Note that this file is quite large so we did not include it in this repository.)
`5_3_shuffle.sh` will shuffle the `5_2_clone_sets.ndjson` file, and extract the first 1000 items into `5_4_shuffled.ndjson`.
After converting this ndjson file into csv using `util_ndjson_to_csv.py`, we have imported it into a Google Spreadsheet for manual evaluation.

You can find the manual review results in `5_5_evaluated.csv`.

### Evaluation criteria

One basic criterion is: whether the suggestion of iccheck was given to an actual clone set or not.

"Reason" column describes the results we found.
Note that these criteria / reasons are subjective due to the nature of the experiment.

- unrelated: Not a clone / these locations are clearly changed for a different reason
- auto generated: Not a clone / these locations are generated by a program
- cloned (but consistent / inconsistent): A clone (but consistent or inconsistent)
    - This "reason" includes instances where one file is auto-generated (compiled) from the other file.
